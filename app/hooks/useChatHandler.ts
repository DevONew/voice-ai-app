'use client'

import { useCallback, useRef, useEffect } from 'react'
import { ConversationHistory } from '../types'
import { useAudioAPI } from './useAudioAPI'
import { detectLanguage } from '../utils/language-detector'

interface UseChatHandlerProps {
  conversationHistory: ConversationHistory
  onResponseReceived: (response: string) => void
  onStateChange: (state: 'processing' | 'speaking') => void
  onLanguageDetected: (language: string) => void
  onError: () => void
  onAudioGenerated: (audioBlob: Blob) => void
  onPlayStart: () => void
}

/**
 * Chat API í˜¸ì¶œ, ì–¸ì–´ ê°ì§€, TTS ì²˜ë¦¬ë¥¼ ë‹´ë‹¹í•˜ëŠ” ì»¤ìŠ¤í…€ í›…
 */
export function useChatHandler({
  conversationHistory,
  onResponseReceived,
  onStateChange,
  onLanguageDetected,
  onError,
  onAudioGenerated,
  onPlayStart,
}: UseChatHandlerProps) {
  const { handleChatAPI, handleTTSAPI } = useAudioAPI()
  const conversationHistoryRef = useRef(conversationHistory)

  // conversationHistory ë³€ê²½ë  ë•Œë§ˆë‹¤ ref ì—…ë°ì´íŠ¸
  useEffect(() => {
    conversationHistoryRef.current = conversationHistory
  }, [conversationHistory])

  const MAX_SENTENCE_LENGTH = 20

  // 20ê¸€ì ì´ˆê³¼ ë¬¸ì¥ì„ ë” ì‘ì€ ë¶€ë¶„ìœ¼ë¡œ ë¶„ì ˆ
  const splitLongSentence = (sentence: string): string[] => {
    // ì‰¼í‘œë¡œ ë‚˜ëˆ„ê¸° ì‹œë„
    const parts = sentence.split(',')
    const result: string[] = []

    parts.forEach((part, idx) => {
      const text = part.trim() + (idx < parts.length - 1 ? ',' : '')
      if (text.length <= MAX_SENTENCE_LENGTH) {
        result.push(text)
      } else {
        // ì‰¼í‘œë„ ì•ˆë˜ë©´ ê³µë°±ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
        const words = text.split(' ')
        let chunk = ''
        words.forEach((word) => {
          if ((chunk + word).length <= MAX_SENTENCE_LENGTH) {
            chunk += (chunk ? ' ' : '') + word
          } else {
            if (chunk) result.push(chunk)
            chunk = word
          }
        })
        if (chunk) result.push(chunk)
      }
    })

    return result
  }

  // ë¬¸ì¥ì„ êµ¬ë¶„ìë¡œ ë‚˜ëˆ„ëŠ” í•¨ìˆ˜
  const splitSentences = (text: string): string[] => {
    // 1ë‹¨ê³„: ë¬¸ì¥ ëìœ¼ë¡œ ë¶„ì ˆ
    const sentences = text
      .split(/([.!?ã€‚ï¼ï¼Ÿ\n]+)/g) // êµ¬ë¶„ì í¬í•¨
      .reduce((result: string[], item: string, index: number, arr: string[]) => {
        if (index % 2 === 0 && item.trim()) {
          result.push(item.trim())
        } else if (index % 2 === 1 && index > 0 && arr[index - 1].trim()) {
          if (result.length > 0) {
            result[result.length - 1] += item
          }
        }
        return result
      }, [])
      .filter((s: string) => s.trim().length > 0)

    // 2ë‹¨ê³„: 20ê¸€ì ì´ìƒì´ë©´ ì¶”ê°€ ë¶„ì ˆ
    return sentences.flatMap(sentence =>
      sentence.length <= MAX_SENTENCE_LENGTH ? [sentence] : splitLongSentence(sentence)
    )
  }

  // ê° ë¬¸ì¥ì„ 3ê°œì”© ë°°ì¹˜ë¡œ ë‚˜ëˆ ì„œ TTS ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
  const processSentencesTTS = async (sentences: string[]) => {
    try {
      const audioBlobs: Blob[] = []
      const BATCH_SIZE = 3

      // 3ê°œì”© ë°°ì¹˜ë¡œ ë‚˜ëˆ ì„œ ì²˜ë¦¬
      for (let i = 0; i < sentences.length; i += BATCH_SIZE) {
        const batch = sentences.slice(i, i + BATCH_SIZE)
        console.log(`ğŸµ ë°°ì¹˜ ì²˜ë¦¬ (${Math.floor(i / BATCH_SIZE) + 1}/${Math.ceil(sentences.length / BATCH_SIZE)}): ${batch.length}ê°œ ë¬¸ì¥`)

        // ë°°ì¹˜ ë‚´ì˜ ë¬¸ì¥ë“¤ì„ ë™ì‹œì— ì²˜ë¦¬
        const batchPromises = batch.map((sentence) => {
          console.log(`  â””â”€ ë¬¸ì¥ TTS ì²˜ë¦¬: "${sentence}"`)
          return handleTTSAPI(sentence)
        })

        const batchResults = await Promise.all(batchPromises)
        audioBlobs.push(...batchResults)
      }

      // Blob ë³‘í•©
      const mergedBlob = new Blob(audioBlobs, { type: 'audio/mpeg' })
      console.log(`ğŸµ ëª¨ë“  ë¬¸ì¥ TTS ì²˜ë¦¬ ì™„ë£Œ (${sentences.length}ê°œ ë¬¸ì¥)`)

      return mergedBlob
    } catch (err) {
      console.error('âŒ ë¬¸ì¥ TTS ì²˜ë¦¬ ì—ëŸ¬:', err)
      throw err
    }
  }

  const handleFinalTranscript = useCallback(
    (finalText: string) => {
      console.log('ğŸ“¤ ë°±ê·¸ë¼ìš´ë“œì—ì„œ Chat API í˜¸ì¶œ:', finalText)
      console.log('ğŸ“‹ í˜„ì¬ conversationHistory:', conversationHistoryRef.current)

      // ì‚¬ìš©ì ì…ë ¥ì—ì„œ ì–¸ì–´ ê°ì§€
      const detectedLanguage = detectLanguage(finalText)
      if (detectedLanguage) {
        console.log(`ğŸŒ ì–¸ì–´ ê°ì§€: ${finalText} â†’ ${detectedLanguage}`)
        onLanguageDetected(detectedLanguage)
      }

      // Promiseë¡œ í˜¸ì¶œ (ê¸°ë‹¤ë¦¬ì§€ ì•ŠìŒ)
      handleChatAPI(finalText, conversationHistoryRef.current, (newHistory) => {
        // conversationHistory ì—…ë°ì´íŠ¸ëŠ” page.tsxì—ì„œ ì²˜ë¦¬
      })
        .then(async (aiResponse) => {
          console.log('âœ… Chat API ì‘ë‹µ (ë°±ê·¸ë¼ìš´ë“œ):', aiResponse)
          onResponseReceived(aiResponse)

          // ë¬¸ì¥ ë‹¨ìœ„ë¡œ TTS ì²˜ë¦¬
          try {
            const sentences = splitSentences(aiResponse)
            console.log(`ğŸµ TTS ì²˜ë¦¬ ì‹œì‘ (${sentences.length}ê°œ ë¬¸ì¥):`, sentences)

            const audioBlob = await processSentencesTTS(sentences)
            console.log('ğŸµ TTS ì²˜ë¦¬ ì™„ë£Œ')
            onAudioGenerated(audioBlob)

            // TTS ì™„ë£Œ í›„ ì¬ìƒ ì‹œì‘
            setTimeout(() => {
              console.log('ğŸ¯ ìƒíƒœ ë³€ê²½: processing â†’ speaking')
              onStateChange('speaking')
              onPlayStart()
            }, 500)
          } catch (ttsErr) {
            console.error('âŒ TTS ì—ëŸ¬:', ttsErr)
            onError()
          }
        })
        .catch((err) => {
          console.error('âŒ Chat API ì—ëŸ¬ (ë°±ê·¸ë¼ìš´ë“œ):', err)
          onError()
        })
    },
    [handleChatAPI, handleTTSAPI, onResponseReceived, onStateChange, onLanguageDetected, onError, onAudioGenerated, onPlayStart]
  )

  return { handleFinalTranscript }
}
